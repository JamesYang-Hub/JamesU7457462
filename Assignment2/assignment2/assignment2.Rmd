---
title: "Assignment2"
author: "U7457462_Sihao_Yang"
date: "2022-10-25"
output: html_document
---


# Assignment2: Statistical Analysis and Interpretation 
(Github Repository: https://github.com/JamesYang-Hub/JamesU7457462)

## 1. Correct analysis of *Clark et al. (2020)* data to generate the summary statistics (means, SD, N) for each of the fish species’ average activity for each treatment.
### Libraries and Packages

Load in the corresponding libraries, `tidyverse`, `readxl`, `metafor`,`metadat` and so on.
```{r}
library(tidyverse)
library(readr)
install.packages("pacman", repos = "http://cran.us.r-project.org")
library(pacman)
p_load(bookdown, tidyverse, ggforce, flextable, latex2exp, png, magick) # list all the packages 
library(puniform) #Load puniform 
library(metadat)
library(metafor)
library(orchaRd)


```

### Import CSV files
The data is in an Excel file (“OA_activitydat_20190302_BIOL3207.csv”) 
```{r}
data_Clark_path <- "data/OA_activitydat_20190302_BIOL3207.csv"
data_Clark <- read_csv(data_Clark_path)
head(data_Clark)
```

### Remove the NAs
First, write some code below to remove missing data
```{r}
# Code to removing missing data from the `OA_activitydat_20190302_BIOL3207.csv` data frame. 
data_Clark <- na.omit(data_Clark)
```

### Create the table of summary statisitics of mean/SD/N of data
```{r sumtable, echo=TRUE, eval=TRUE, tab.cap = "sumtable"}
# Use flextable to render the summary table in a tidy format
mean_sd <- list(mean=~mean(.x, na.rm = TRUE), 
  sd = ~sd(.x, na.rm = TRUE)
)
data_Clark_mean_sd <- data_Clark %>% select(species,treatment,sl,size,activity) %>% group_by(species,treatment,size)%>% summarise(across(where(is.numeric), mean_sd),N=n()) 
data_Clark_mean_sd%>% flextable(,col_keys = c("species","treatement","size","sl_mean","sl_sd","activity_mean","activity_sd","N")) %>% add_header_row(, values = c("", "sl","activity",""), colwidths = c(3, 2,2,1)) %>% align(, i = 1, part = "header", align = "center")
```

## 2.Through coding, merge the summary statistics generated from 1) with the metadata  from *Clark et al. (2020)*.
### Make the tibble tidy and rename the columns
```{r actable, echo=TRUE, eval=TRUE, tab.cap = "actable"}
# calculate mean/SD/N of activity group by treatment
data_Clark_activity_mean_sd <- data_Clark %>% select(species,treatment,activity) %>% group_by(species,treatment)%>% summarise(across(where(is.numeric), mean_sd),N=n()) 
# use `pivot_wider()` to separate the activity value of treatment
data_Clark_activity_mean_sd <- data_Clark_activity_mean_sd %>% pivot_wider(names_from = treatment,values_from =c(activity_mean,activity_sd,N) ) 
# rename the columns
names(data_Clark_activity_mean_sd) <-c('Species','oa.mean','ctrl.mean','oa.sd','ctrl.sd','oa.n','ctrl.n')
data_Clark_activity_mean_sd
```
### Read the `ocean_paper_data.csv` and merge with *Clark's* data
```{r}
# read the CSV file
paper_path <- "./data/clark_paper_data.csv"
ocean_paper <- read_csv(paper_path)
# replace NA with '-'
ocean_paper <- ocean_paper %>% mutate_at('Cue/stimulus type', ~replace(., is.na(.), '-'))
# merge the tibbles into the metadata 
Clark_meta_data <- cbind(ocean_paper,data_Clark_activity_mean_sd) 

```

## 3. Through coding, correctly merge the combined summary statistics and metadata from *Clark et al. (2020)* (output from 1 & 2) into the larger meta-analysis dataset .
### Import and merge the file
```{r metatable, echo=TRUE, eval=TRUE, tab.cap = "metatable"}
# read the file
ocean_meat_path <- "./data/ocean_meta_data.csv"
ocean_meat_data <- read_csv(ocean_meat_path)
# merge with Clark_meata_data
ocean_meta_data <- rbind(ocean_meat_data,Clark_meta_data) 
# eliminate some unresonable data
ocean_meta_data <- ocean_meta_data %>% filter(ctrl.mean>0,oa.mean>0)
colnames(ocean_meta_data)[14] <- 'sti_type'
# Add in observation-level (residual) column. This column simply counts, from 1 to the number of rows
ocean_meta_data <- ocean_meta_data %>% mutate(residual = 1:n()) # Add this observation level variable. 
head(ocean_meta_data)
```




## 4. Correctly calculate the log response ratio (lnRR) effect size for every row of the dataframe using metafor’s `escalc()` function.
### Use `escalc` function with the measure 'ROM'
```{r}
# calculate the log response ratio (lnRR) effect size
lnRR <- metafor::escalc(measure='ROM',
                        n1i = ocean_meta_data$ctrl.n ,n2i=ocean_meta_data$oa.n ,
                        m1i=ocean_meta_data$ctrl.mean ,m2i= ocean_meta_data$oa.mean ,
                        sd1i=ocean_meta_data$ctrl.sd,sd2i=ocean_meta_data$oa.sd,
                        var.names=c("yi","vi"),data = ocean_meta_data,
                        slab=paste(ocean_meta_data$Authors,ocean_meta_data$`Year (online)`,sep = ","))
view(lnRR)# view the lnRR data

```

## 5. Correct meta-analytic model fitted to the data that controls for the sampling variance of lnRR. The model should include a random effect of study and observation. Use metafor’s `rma.mv()` function.
### Create the model
```{r MLMAable, echo=TRUE, eval=TRUE, tab.cap = "MLMAtable"}
# use `rma.mv()` function with random effect of 'Study' and 'residual'
MLMA <- metafor::rma.mv(lnRR$yi,lnRR$vi,random = list(~1|Study,~1|residual),test="t",data = ocean_meta_data)
MLMA

```

## 6.Written paragraph of the findings and what they mean which is supported with a figure. The paragraph should include:



### Correct presentation and interpretation of overall meta-analytic mean and measures of uncertainty around the mean estimate (e.g., 95% confidence intervals).
#### Use `predict()` function
```{r pre1table, echo=TRUE, eval=TRUE, tab.cap = "pre1table"}
pre1 <- predict(MLMA,digits = 3)
pre1
```
Uncertainty in the overall meta-analytic mean: 95% Confidence Intervals is (`pre1$ci.lb`,`pre1$ci.ub`)

#### Plot the predictions and see if they are inside the confidence interval
```{r 95CIfig, fig.align='center', fig.cap="95CIfig"}
# qq-normal plots
res <- rma(yi, vi, data =lnRR)
qqnorm(res, main = "Random-Effects Model")

```
To check whether the data violate the normal distribution assumption of the random effects model
The points are not all within the dotted line(95% CI), which means they do not conform to a normal distribution




### Measures of heterogeneity in effect size estimates across studies (i.e., I^2 and/or prediction intervals - see `predict()` function in metafor)
```{r I2table, echo=TRUE, eval=TRUE, tab.cap = "I2table"}

i2_vals <- orchaRd::i2_ml(MLMA)
i2 <- tibble(type = firstup(gsub("I2_", "", names(i2_vals))), I2 = i2_vals)
flextable(i2) %>%
    align(part = "header", align = "center") %>%
    compose(part = "header", j = 1, value = as_paragraph(as_b("Type"))) %>%
    compose(part = "header", j = 2, value = as_paragraph(as_b("I"), as_b(as_sup("2")),
        as_b("(%)")))

```
It can be seen that I^2 is `i2$I2[1]` %, which is highly heterogeneous, and a random effect model can be used.
Overall, we have highly heterogeneous effect size data because sampling variation doesn't contributes to the total variation in effects.\@ref(tab:I2table)
From the multilevel meta-analytic model we find that only `i2$I2[2]`% of the total variation in effect size estimates is the result of differences between studies.


```{r pretable, echo=TRUE, eval=TRUE, tab.cap = "pretable"}
pre1 <- predict(MLMA)
pre1
```
The prediction intervals is (`pre1$pi.lb`,`pre1$pi.ub`)



### Forest plot showing the mean estimate, 95% confidence interval, and prediction interval with clearly labelled axes, number of samples and studies plotted on figure
#### Make forest plot
```{r forefig, fig.align='center', fig.cap="forefig"}
forest(MLMA,slab = paste(ocean_meta_data$Authors, 
                    as.character(ocean_meta_data$`Year (online)`), sep = ", "),  header="Author(s) and Year")

```
```{r orchfig, fig.align='center', fig.cap="orchfig"}

orchaRd::orchard_plot(MLMA, mod="1", data = ocean_meta_data, group = "Study", xlab = "Standardised mean difference", 
  transfm = "none") 

```

Orchard plot showing the standardised mean difference for correlation coefficients estimated between Study.\@ref(fig:orchfig)


## 7. Funnel plot for visually assessing the possibility of publication bias.

### Make funnel plot
```{r fun1fig, fig.align='center', fig.cap="fun1fig"}
# make a funnel plot to visualize the data in relation to the precision,
# inverse sampling standard error,
metafor::funnel(x = lnRR$yi, vi = lnRR$vi, yaxis = "seinv",
    digits = 2, level = c(0.1, 0.05, 0.01), shade = c("white", "gray55", "gray 75"),
    las = 1, xlab = "Correlation Coefficient (r)", atransf = tanh, legend = TRUE,xlim=c(-6,6),ylim=c(1,125))
```



### Fitting a Multilevel Meta-Regression model to Test and Correct for Publication bias

```{r MLMA2table, echo=TRUE, eval=TRUE, tab.cap = "MLMA2table"}
MLMA2 <- metafor::rma.mv(lnRR$yi,lnRR$vi,
                         random = ~Study|residual,
                         mods = ~`Year (online)`,
                         test="t",data = ocean_meta_data)
MLMA2

```

```{r fun2fig, fig.align='center', fig.cap="fun2fig"}

funnel(MLMA2,xlim=c(-5,5),ylim=c(0,4))
# Lets make a funnel plot to visualize the data in relation to the precision, inverse sampling standard error,
metafor::funnel(MLMA,refline=0,xlim=c(-2,2),ylim=c(0,1), level=c(90, 95, 99), 
       shade=c("white", "gray", "darkgray"),  legend="topleft")  

```
This plot show us that some studies are lying in the 'significance' contours. From the figure, the figure has good left-right symmetry, so there may be no publication bias. Studies with large sample sizes, distributed at the top of the funnel plot, and concentrated toward the middle.



```{r fun3fig, fig.align='center', fig.cap="fun3fig"}
# flip around the funnel plot
ggplot(lnRR, aes(y = lnRR$yi, x = lnRR$vi)) + geom_point() + geom_smooth(method = lm) +
    labs(y = "Fisher's Z-transformed Correlation Coefficient (Zr)", x = "Sampling Variance of Zr") +
    theme_classic()+xlim(c(0,20))
```
We can see from the linear model fit that there appears to be a clear positive slope where the mean effect size is dragged ‘up’ when the sampling variance is large. This is caused by the excessive heterogeneity. These studies are hard to publish or authors are just less likely to believe them, and therefore not publish them.\@ref(fig:fun3fig)

```{r ranktable, echo=TRUE, eval=TRUE, tab.cap = "ranktable"}
# Detection of publication bias
ranktest(MLMA)
```
The rank correlation test detects whether the observed effect size is significantly related to the corresponding sample variance. The correlation is significant, which means that there are small-study effects(small sample size & large effect size).



## 8.Time-lag plot assessing how effect sizes may or may not have changed through time.
```{r timelagfig, fig.align='center', fig.cap="timelagfig"}
ggplot(lnRR, aes(y = yi, x = ocean_meta_data$`Year (online)`, size = 1/sqrt(vi))) + geom_point(alpha = 0.3) +
    geom_smooth(method = lm, col = "red", show.legend = FALSE) + labs(x = "Publication Year",
    y = "Correlation Coefficient ", size = "Precision (1/SE)") +
    theme_classic()
```
There does appear to be a clear positive relationship with year. The year studies have the same sampling variance (i.e., lower precision). 

```{r timetable, echo=TRUE, eval=TRUE, tab.cap = "timetable"}
# Quantifying time lag bias using multilevel meta-regression

metareg_time <- rma.mv(lnRR$yi ~ ocean_meta_data$`Year (online)`, V = lnRR$vi, random = list(~1 | Study, ~1 |residual),test = "t", dfs = "contain", data = ocean_meta_data)# Including sampling variance as moderator
summary(metareg_time)
```

## 9.Formal meta-regression model that includes year as a moderator (fixed effect) to test for time-lag bias
```{r modtable, echo=TRUE, eval=TRUE, tab.cap = "modtable"}
MLMA_year <- metafor::rma.mv(lnRR$yi,lnRR$vi,mods=`Year (online)`,random = list(~1|Study,~1|residual),test="t",data = ocean_meta_data)
MLMA_year
```

```{r timelagtable, echo=TRUE, eval=TRUE, tab.cap = "timelagtable"}
r2_time <- orchaRd::r2_ml(metareg_time)  
r2_time
# Time-lag explains 2.48% of the variation in effect size yi.
```
Time-lag explains `100*r2_time[1]`% of the variation in effect size.




## 10.Formal meta-regression model that includes inverse sampling variance (i.e., 1/(lnRR)) to test for file-drawer biases
```{r modinvtable, echo=TRUE, eval=TRUE, tab.cap = "modinvtable"}
# Including inverse sampling variance and year as moderators to account for file-drawer biases
MLMA_inverse <- metafor::rma.mv(lnRR$yi,lnRR$vi,
                         random = list(~1|Study,~1|residual),
                         mods = ~(1/lnRR$vi),
                         test="t",data = ocean_meta_data)

summary(MLMA_inverse)
```


```{r inversetable, echo=TRUE, eval=TRUE, tab.cap = "inversetable"}
r2_time_inv <- orchaRd::r2_ml(MLMA_inverse) 
r2_time_inv
# There is no evidence accounting for file-drawer biases
```


## 11.A written paragraph that discusses the potential for publication bias based on the meta-regression results. What type of publication bias, if any, appears to be present in the data? If publication bias is present, what does it mean and what might be contributing to such bias?

```{r}
# Based on the results of the meta-regression, there is a potential lag bias. That occurs when the speed of publication of the studies included in a review, depends on the direction and strength of their results. Those studies which report significant results, or strong associations, may be published sooner than those that report non-significant results, or weak associations.
# There is also the impact of impact factor on publication bias, because articles with strong correlation are more likely to be published in high-impact journals, that is, articles with strong effect sizes have a higher impact factor, which also leads to publication bias. reason.
```

## 12.Identify any studies contributing to publication bias. How do your updated meta-analysis results compare with a meta-analysis by Clement et. al. (2022)? Are there any concerns about these studies? If so, describe using references to existing papers what concerns have been raised?

```{r}
# The article by *Munday et al.(2010)* may have contributed to article bias because some of the studies in the article had too small effect sizes.
# *Clement et. al. (2022)* meta-analysis focused on the declining impact of ocean acidification and fish activity over a decade, and because of issues in some journals, high-impact studies are more likely to be published in journals with higher impact factors, There are others that contribute to publication bias.
# My analysis is for time and heterogeneity between samples for the articles, and as with these articles, all results point to a convergence in the effects of seawater acidification on fish activity in recent years. Some correlations or significance may be deliberately exaggerated for these studies in order to be published in more influential journals. Moreover, high impact journals can also lead to high citation rates, which can also lead to biased articles and can receive strong publicity that can be overspent on this. (*Clement et. al. (2022)*)
```




